# topo-talk


Setting up debian server for SSH so that Ubuntu VM can access
sudo apt update && sudo apt install openssh-server -y

# ğŸš€ Containerlab RAG-Powered Network Chatbot

## ğŸ“Œ Overview
This project is a **RAG-based network topology chatbot** that:
- ğŸ–¥ï¸ **Analyzes network topologies from Containerlab.**
- ğŸ“± **Retrieves startup configs over SSH using Paramiko.**
- ğŸ“‚ **Uses ChromaDB for vector-based retrieval.**
- ğŸ§  **Generates topology-aware answers using Mistral LLM.**

ğŸ’¡ **Use Case:**  
Provides insights into network **device connectivity, routing, and configurations** from startup configs of **Cisco, Arista, and other vendors**.

---

## âš™ï¸ **How It Works**
### ğŸ§ **Step-by-Step Process**
1ï¸âƒ£ **User Inputs Containerlab File:**  
   - Provide the topology file (`.yaml`) via the Streamlit UI.  

2ï¸âƒ£ **Topology Data Collection:**  
   - Uses **Paramiko SSH** to fetch startup configurations from network devices.  
   - Data is stored in JSON format.

3ï¸âƒ£ **Vector Storage (ChromaDB):**  
   - Splits configs into **semantic chunks** using `SemanticChunker`.  
   - Stores them as embeddings for retrieval.  

4ï¸âƒ£ **Retrieval-Augmented Generation (RAG):**  
   - **MultiQueryRetriever** generates alternative queries.  
   - Uses **MMR (Maximal Marginal Relevance)** to fetch diverse but relevant context.  

5ï¸âƒ£ **LLM Query Processing:**  
   - **Ollama's Mistral model** processes topology-related queries.  
   - Responses are structured with network-specific insights.  

---

## ğŸš€ **Quick Start Guide**
### âš–ï¸ **1. Install Dependencies**
```sh
pip install -r requirements.txt
```

### ğŸ¢ **2. Set Up Ollama LLM**
```sh
ollama serve
```

### ğŸ’¾ **3. Run the Chatbot**
```sh
streamlit run app.py
```

---

## âš™ï¸ **Configuration**
The chatbot retrieves settings from `.env`.  
Create a `.env` file with:
```ini
BASE_LLM_URL=http://localhost:11434
```

Modify model settings via the **Streamlit sidebar UI**:
- **Temperature**: Adjusts response accuracy vs. creativity.
- **MMR Retrieval Settings**: Tuned for **balanced context relevance**.

---
---

## ğŸ›  **Troubleshooting & Debugging**
### âš ï¸ **1. Event Loop Closed Error**
**Issue:**  
`RuntimeError: Event loop is closed`  
**Fix:**  
Ensure **one persistent event loop**:
```python
if 'loop' not in st.session_state:
    st.session_state['loop'] = asyncio.new_event_loop()
    asyncio.set_event_loop(st.session_state['loop'])
```

---

### âš ï¸ **2. VectorDB Not Found**
**Issue:**  
`ChromaDB: No database found at vectorstore_db/`  
**Fix:**  
- Ensure `topology_data/` contains valid JSON files.
- Rebuild the vectorstore:
```python
chatbot.store_in_chroma()
```

---

### âš ï¸ **3. LLM Not Responding**
**Issue:**  
LLM fails to process queries.  
**Fix:**  
- Check if **Ollama is running**:
  ```sh
  ollama serve
  ```
- Verify `.env` has the **correct BASE_LLM_URL**.

---

## ğŸ† **Key Features**
âœ… **Supports Cisco, Arista, and Other Vendors**  
âœ… **Uses MMR + MultiQuery Retrieval for Better Accuracy**  
âœ… **Persistent Chat Memory** (Streamlit session state)  
âœ… **Dynamic LLM Temperature Control**  

---

## ğŸ¯ **Future Improvements**
ğŸ”¹ **Deploy to Cloud (GCP/AWS)**  
ğŸ”¹ **Support More Network Vendors (Juniper, Palo Alto)**  
ğŸ”¹ **Optimize Retrieval Speed (FAISS for Faster Queries)**  

---

## ğŸ“ˆ **Credits & Contributors**
- Developed by **[Your Name]**  
- **Special Thanks** to OpenAI, LangChain, and Containerlab teams  

---

ğŸ”¹ **Feel free to fork & contribute!** ğŸš€

